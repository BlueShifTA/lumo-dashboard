# Lumo Dashboard â€” Specification
**Version:** 0.1  
**Date:** 2026-02-20  
**Status:** Draft â€” Ready for Implementation  
**Repo:** BlueShifTA/lumo-dashboard

See `CLAUDE.md` for the primary project guide and current repo/runtime conventions.

---

## 1. Overview

A real-time web dashboard for monitoring and controlling Beluga's physical body:
- SO-ARM101 6-DOF robot arm
- IMX219 stereo camera
- Live joint telemetry + task execution UI

**Access:** `http://orin-home.beluga-buri.ts.net:8002` (Tailscale, port 8002)

---

## 2. Hardware Reference

### Robot Arm: SO-ARM101
- **Port (follower):** `/dev/ttyACM1`
- **Port (leader/teleoperation):** `/dev/ttyACM0`
- **Calibration:** `~/.cache/huggingface/lerobot/calibration/robots/so_follower/beluga_follower_arm.json`
- **Library:** `lerobot` â†’ `SOFollower` / `SOFollowerRobotConfig`
- **Control env:** `worklerobot` alias (activates LeRobot virtualenv)

#### Joint Map (6-DOF)
| Joint | Name | Range |
|-------|------|-------|
| J1 | shoulder_pan | Â±180Â° |
| J2 | shoulder_lift | Â±90Â° |
| J3 | elbow_flex | Â±135Â° |
| J4 | wrist_flex | Â±90Â° |
| J5 | wrist_roll | Â±180Â° |
| J6 | gripper | 0â€“100% |

### Camera: IMX219 Stereo
- **Device:** `/dev/video0` (NVIDIA ISP5)
- **Pipeline:** `nvarguscamerasrc â†’ GStreamer â†’ nvvidconv â†’ RGB`
- **Driver:** NVIDIA tegra-camera (NOT V4L2 â€” returns broken Bayer data)
- **Resolution:** 1920Ã—1080 @ 30 FPS
- **Service:** `BelugaCamera` class (thread-safe, production script at `/home/nvidia/scripts/production/camera_service.py`)

### Compute
- **Platform:** Jetson Orin Nano 8GB (arm64)
- **Remote:** `orin-home.beluga-buri.ts.net` via Tailscale

---

## 3. Architecture

```
Browser (Remote via Tailscale)
         â”‚
         â–¼
FastAPI Backend (:8002)
â”œâ”€â”€ /arm/*      â†’ SOFollower control (lerobot subprocess)
â”œâ”€â”€ /camera/*   â†’ BelugaCamera (GStreamer)
â”œâ”€â”€ /ws/telemetry â†’ WebSocket: 10Hz joint + system data
â””â”€â”€ /tasks/*    â†’ Task queue + execution
         â”‚
         â”œâ”€â”€ arm_driver.py  (wraps lerobot SOFollower)
         â””â”€â”€ camera_service.py (existing production script)
```

---

## 4. API Endpoints

### Arm Control
```
GET  /arm/status        â†’ {joints: {name: angle}, connected: bool, temp: float}
POST /arm/move          â†’ {joints: {name: float}, speed: 0-100} â†’ {ok, eta_ms}
POST /arm/home          â†’ Move all joints to home position (0Â°)
POST /arm/stop          â†’ Emergency stop (hold current position)
GET  /arm/calibration   â†’ Return calibration data
```

### Camera
```
GET  /camera/frame      â†’ Latest RGB frame (JPEG)
GET  /camera/stream     â†’ MJPEG stream
GET  /camera/status     â†’ {connected, fps, resolution}
```

### WebSocket
```
WS /ws/telemetry        â†’ Push every 100ms:
{
  "ts": "ISO8601",
  "joints": {
    "shoulder_pan": {"pos": 45.2, "target": 45.0, "load": 0.12},
    "shoulder_lift": {...},
    ...
  },
  "gripper": {"pos": 50.0, "load": 0.08},
  "system": {"cpu": 23.4, "gpu": 41.2, "temp_cpu": 48.1, "temp_gpu": 52.3}
}
```

### Tasks
```
GET  /tasks             â†’ List defined tasks
POST /tasks/{name}/run  â†’ Execute named task
GET  /tasks/{name}/status â†’ {running, progress, log}
POST /tasks/custom      â†’ {script: "python3 ...", args: [...]}
```

---

## 5. Frontend Panels

### Panel 1: Connection Status Bar (top)
- Arm: ðŸŸ¢ Connected / ðŸ”´ Disconnected
- Camera: ðŸŸ¢ Streaming / ðŸ”´ Offline
- Port indicators: `/dev/ttyACM1`, `/dev/video0`
- Jetson temp: CPU/GPU Â°C

### Panel 2: Joint Telemetry (center-left)
- 6 joint dials â€” live angle, color-coded by load
- Numeric readout: angle in degrees + load %
- Update rate: 10 Hz via WebSocket
- History chart: last 30s per joint (sparkline)

### Panel 3: Camera Feed (center-right)
- MJPEG stream from `/camera/stream`
- Toggle: RGB / (future: IR thermal)
- FPS overlay

### Panel 4: 3D Arm Visualization (optional, phase 2)
- Three.js or simple SVG 2D side-view
- Animate joints from WebSocket data

### Panel 5: Task Execution (bottom)
- Predefined tasks (buttons):
  - ðŸ  Home â€” all joints to 0Â°
  - ðŸ›‘ Emergency Stop
  - ðŸ“ Wave (demo sequence)
  - ðŸ¤ Gripper Open/Close
  - ðŸŽ¯ Move to Pose (custom joint input)
- Custom script input (advanced)
- Task log: last 10 executions

---

## 6. Tech Stack

| Layer | Choice | Reason |
|-------|--------|--------|
| Backend | FastAPI + Python | Same as LumoRobotic, matches your stack |
| WebSocket | FastAPI native | Built-in, no extra deps |
| Frontend | Next.js 15 + React 18 | Template base |
| Styling | Tailwind CSS | Template base |
| Charts | Recharts | Lightweight, already in pravafin |
| MJPEG | Direct GET stream | Simplest for camera feed |
| Scaffold origin | BlueShifTA/fastapi-nextjs-template | Historical source of the initial repo layout |

---

## 7. File Structure

```
lumo-dashboard/
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ devops/
â”œâ”€â”€ docs/
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ arm.py          â† arm endpoints + WebSocket
â”‚   â”‚   â”‚   â”œâ”€â”€ camera.py       â† camera endpoints
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py        â† task queue
â”‚   â”‚   â”œâ”€â”€ package/
â”‚   â”‚   â”‚   â”œâ”€â”€ arm_driver.py   â† SOFollower wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ camera_driver.pyâ† BelugaCamera wrapper
â”‚   â”‚   â”‚   â””â”€â”€ task_runner.py  â† subprocess task execution
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â””â”€â”€ frontend/
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ app/
â”‚           â”‚   â””â”€â”€ page.tsx    â† Dashboard layout
â”‚           â””â”€â”€ components/
â”‚               â”œâ”€â”€ JointPanel.tsx
â”‚               â”œâ”€â”€ CameraFeed.tsx
â”‚               â”œâ”€â”€ TaskPanel.tsx
â”‚               â””â”€â”€ StatusBar.tsx
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ justfile                    â† dev commands
â””â”€â”€ README.md
```

---

## 8. Safety Rules (Non-Negotiable)

- **Emergency stop** always visible â€” single click, no confirmation
- **Joint limits enforced in software** â€” backend rejects out-of-range moves
- **Connection watchdog** â€” if WS drops for >2s, arm holds position (no drift)
- **No auto-reconnect loop** â€” arm stays put until human confirms reconnect
- **Gripper never auto-closes** â€” only explicit commands

---

## 9. Implementation Phases

### Phase 1 â€” MVP (3-4 days)
- [x] Initialize repo from FastAPI/Next.js scaffold (completed)
- [ ] Backend: arm status + basic move + emergency stop
- [ ] Backend: camera frame endpoint
- [ ] Frontend: Status bar + joint readout (polling, not WS yet)
- [ ] Frontend: Camera feed panel
- [ ] Frontend: Home + E-stop buttons
- **Success:** Can read joints, see camera, send home command

### Phase 2 â€” Real-time (2-3 days)
- [ ] WebSocket telemetry (10 Hz)
- [ ] Joint sparkline charts
- [ ] Task queue + predefined tasks
- [ ] Custom joint pose input
- **Success:** Live updates without page refresh

### Phase 3 â€” Polish (2 days)
- [ ] 3D/2D arm visualization
- [ ] Task logging + history
- [ ] Mobile-responsive layout
- [ ] Dark mode

---

## 10. RAG & Token Budget Note

When working on this project:
- Workspace context for this project should be in `/home/nvidia/.openclaw/workspace/lumo-dashboard/`
- RAG will auto-index it â€” relevant hardware specs will be retrieved without full-context loading
- Keep implementation files lean; heavy docs go in `docs/` (excluded from RAG context injection)

---

## 11. Open Questions

- [ ] Should tasks run as subprocesses or in-process threads?
- [ ] Do we want LeRobot teleoperation mode in the dashboard (record episodes)?
- [ ] IR thermal camera â€” include in Phase 1 or later?
- [ ] Auth? (Tailscale-only access, so probably no auth needed)
